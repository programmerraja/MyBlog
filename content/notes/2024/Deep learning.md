+++
title = 'Deep learning'
date = 2024-06-08T05:40:06.066+05:30
draft = true
tags =[]
+++ 



##  Perceptron


bias ->The addition of bias reduces the variance and hence introduces flexibility and better generalisation to the neural network.
#### **Activation Function**
- Linear
- Sigmoid or Logistic Activation Function
- Tanh or hyperbolic tangent Activation Function
- ReLU (Rectified Linear Unit) Activation Function ( most used activation function in the world right now)


#### Framework

The two most popular neural frameworks are: [TensorFlow](http://tensorflow.org/) and [PyTorch](https://pytorch.org/).
## Resources
- https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590
- https://medium.com/fintechexplained/neural-networks-bias-and-weights-10b53e6285da
- https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
- https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6