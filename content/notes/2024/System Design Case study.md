+++
title = 'Case study'
date = 2024-04-20T16:45:15.1515+05:30
draft = true
tags =[]
+++ 


## Database Scalling

### [Shopfiy](https://shopify.engineering/horizontally-scaling-the-rails-backend-of-shop-app-with-vitess)

- they started off by splitting the primary database into separate parts 
- They identified groups of large tables that could exist on separate databases, and used [GhostFerry](https://github.com/Shopify/ghostferry) to move a few tables onto a new database.
-  This scaling approach is referred to as “federation” where tables are stored in different MySQLs
- As the app further grew, they were starting to hit the limit of a single MySQL.disk size take many Terabytes.y. they couldn’t further split the primary database, as that would add more complexity in the application layer, and require cross database transactions.
- They choose Vitess  (Vitess is [an open source](https://github.com/vitessio/vitess) database system abstraction on top of MySQL that provides many benefits ([docs with details)](https://vitess.io/docs/archive/15.0/overview/whatisvitess/ "What is Vitess") )

### Figma

#### [Verticall sharding](https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/) (April 4, 2023 )

They go with vertically partition[[Databases#Database Partitioning]] the database by table(s). Instead of splitting each table across many databases, we would move _groups of tables_ onto their own databases. This proved to have both short- and long-term benefits: Vertical partitioning relieves our original database now, while providing a path forward for horizontally sharding subsets of our tables in the future.

They identify which tables can be split by using  `average active sessions` (AAS) for queries, which describes the average number of active threads dedicated to a given query at a certain point in time. We calculated this information by querying [`pg_stat_activity`](https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-ACTIVITY-VIEW) in 10 millisecond intervals to identify CPU waits associated with a query, and then aggregated the information by table name 

They choose the table which will not do joins and required transactions 

Migration approach
1. Prepare client applications to query from multiple database partitions
2. Replicate tables from original database to a new database until replication lag is near 0
3. Pause activity on original database
4. Wait for databases to synchronize
5. Reroute query traffic to the new database
6. Resume activity

`Note`: To fast down the r [logical replication](https://www.postgresql.org/docs/current/logical-replication.html) on they removed the indexing and add the indexing after everythings compeleted

They used Log Sequence Number (it is a unique identifier assigned to each transaction log entry, representing the order in which changes were made to the database. LSNs are used to track the state of replication and determine whether two databases are synchronized.)

They created new **`Query Routing service`**  will centralize and simplify routing logic as we scale to more partitions.
 
#### [Horizontal sharding](https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/) (March 14, 2024)

 Our first goal was to shard a relatively simple but very high traffic table in production as soon as possible
 
 horizontally sharded groups of related tables into colocations.which shared the same sharding key and physical sharding layout. This provided a friendly abstraction for developers to interact with horizontally sharded tables.

`shard key` -> selected a handful of sharding keys like UserID, FileID, or OrgID. Almost every table at Figma could be sharded using one of these keys.

They group the table in same sharding if they comes under single domain and they have same shard key such that the query will support join and transaction 

`Example:` Imagine Figma has a colo named "UserColo" that includes tables related to user data. Within this colo, there are tables such as "Users", "UserPreferences", and "UserActivity". Each of these tables is sharded based on the UserID, ensuring that data related to a specific user is stored together on the same shard.

**Logical Sharding** and **Physical Sharding**

First they did **Logical Sharding** that involves partitioning or organizing data at the application layer in a way that simulates horizontal sharding without physically distributing the data across multiple shards.

Then after sucess of logical they implement **Physical Sharding** that involves the actual distribution of data across multiple backend database servers 


 `DBProxy service` that intercepts SQL queries generated by our application layer, and dynamically routes queries to various Postgres databases. build with `GO`
 The job is
 - A query parser reads SQL sent by the application and transforms it into an Abstract Syntax Tree (AST).
- A logical planner parses the AST and extracts the query type (insert, update, etc) and logical shard IDs from the query plan.
- A physical planner maps the query from logical shard IDs to physical databases. It rewrites queries to execute on the appropriate physical shard.
- if query does not have shard key it will send to all cluster and aggregate the result.
- If they running query that join two table in different shard they will reject it


## Notion

check [here](https://www.notion.so/blog/sharding-postgres-at-notion) 

- They go with horizontal sharding and **application-level sharding**.
- Partition block data by workspace ID
- **480 logical shards** evenly distributed across **32 physical databases**.

Migratio process

1. **Double-write:** Incoming writes get applied to both the old and new databases.
2. **Backfill:** Once double-writing has begun, migrate the old data to the new database.
3. **Verification:** Ensure the integrity of data in the new database.
4. **Switch-over:** Actually switch to the new database. This can be done incrementally, e.g. double-reads, then migrate all reads.

## Caching

### [DoorDash’s](https://doordash.engineering/2023/10/19/how-doordash-standardized-and-improved-microservices-caching/)

They use Layered caches
1. _Request local cache_: Lives only for the lifetime of the request; uses a simple HashMap.
2. _Local cache_: Visible to all workers within a single Java virtual machine; uses a [Caffeine cache](https://github.com/ben-manes/caffeine) for heavy lifting.
3. _Redis cache_: Visible to all pods sharing the same Redis cluster; uses Lettuce client.

They have Runtime feature flag control to enable and disable the caching in layer

Cache invalidation
- Using Change Data Capture events emitted when database tables are updated
-  The cache could be invalidated directly within the application code when data changes. This is faster but potentially more complex

Cache key how they create unique cache key
1. Unique cache name, which is used as a reference in runtime controls. 
2. Cache key type, a string representing the key’s type of entity to allow categorization of cache keys.
3. ID, a string that refers to some unique entity of cache key type.
4. Configuration, which includes default TTLs and a Kotlin serializer.

To standardize key schema, we chose the uniform resource name ([URN](https://en.wikipedia.org/wiki/Uniform_Resource_Name)) format:

````java
urn:doordash:<cache key type>:<id>#<cache name>
````


## Logging

### **Pinterest**
